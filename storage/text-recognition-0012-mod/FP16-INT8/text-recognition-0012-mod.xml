<?xml version="1.0"?>
<net name="text-recognition-0012" version="11">
	<layers>
		<layer id="0" name="Placeholder" type="Parameter" version="opset1">
			<data shape="1,32,120,1" element_type="f32" />
			<rt_info>
				<attribute name="fused_names" version="0" value="Placeholder" />
				<attribute name="old_api_map_element_type" version="0" value="f16" />
				<attribute name="old_api_map_order" version="0" value="0, 2, 3, 1" />
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="Placeholder,Placeholder:0">
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="1" name="Placeholder/Transpose([0 3 1 2])/value60487212201" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="0" size="32" />
			<rt_info>
				<attribute name="fused_names" version="0" value="Placeholder/Transpose([0 3 1 2])/value60487212" />
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="2" name="Placeholder/Transpose([0 3 1 2])" type="Transpose" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="Placeholder/Transpose([0 3 1 2])" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="3" name="data_mul_53312042829" type="Const" version="opset1">
			<data element_type="f16" shape="1, 1, 1, 1" offset="32" size="2" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="4" name="data_mul_53312041621/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_mul_5331" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="5" name="shadow/BatchNorm/FusedBatchNorm/mean/Fused_Mul_" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm/FusedBatchNorm/mean/Fused_Mul_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="6" name="data_add_53332072889" type="Const" version="opset1">
			<data element_type="f16" shape="1, 1, 1, 1" offset="34" size="2" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="7" name="data_add_53332071622/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_5333" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="8" name="shadow/BatchNorm/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm/FusedBatchNorm:0,shadow/Dropout/Identity:0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="9" name="7357392793" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="36" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="10" name="7367402901" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="40" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="11" name="7377412703" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="36" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="12" name="7387422679" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="40" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="13" name="shadow/Conv/Conv2D/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="14" name="shadow/BatchNorm_1/FusedBatchNorm/mean/Fused_Mul__copy2101623/restored_convert/quantized19852823" type="Const" version="opset1">
			<data element_type="i8" shape="64, 1, 3, 3" offset="44" size="576" />
			<output>
				<port id="0" precision="I8">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="15" name="shadow/BatchNorm_1/FusedBatchNorm/mean/Fused_Mul__copy2101623/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="16" name="shadow/Conv/Conv2D/fq_weights_1/zero_point20042946" type="Const" version="opset1">
			<data element_type="f32" shape="64, 1, 1, 1" offset="620" size="256" />
			<output>
				<port id="0" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="17" name="shadow/Conv/Conv2D/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="18" name="shadow/Conv/Conv2D/fq_weights_1/scale19932751" type="Const" version="opset1">
			<data element_type="f32" shape="64, 1, 1, 1" offset="876" size="256" />
			<output>
				<port id="0" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="19" name="shadow/Conv/Conv2D/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="20" name="shadow/Conv/Conv2D" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="same_upper" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/Conv/Conv2D" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="21" name="data_add_533653412132763" type="Const" version="opset1">
			<data element_type="f16" shape="1, 64, 1, 1" offset="1132" size="128" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="22" name="data_add_533653412131624/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_53365341" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="23" name="shadow/BatchNorm_1/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_1/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm_1/FusedBatchNorm:0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="24" name="shadow/BatchNorm_1/Relu" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_1/Relu" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/BatchNorm_1/Relu:0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="25" name="9919952832" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="26" name="9929962979" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1264" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="27" name="9939972871" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="28" name="9949982874" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1264" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="29" name="shadow/MaxPool2D/MaxPool/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</output>
		</layer>
		<layer id="30" name="shadow/MaxPool2D/MaxPool" type="MaxPool" version="opset8">
			<data strides="2, 2" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" kernel="2, 2" rounding_type="floor" auto_pad="valid" index_element_type="i64" axis="0" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/MaxPool2D/MaxPool" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>32</dim>
					<dim>120</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/Dropout_1/Identity:0,shadow/MaxPool2D/MaxPool:0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
			</output>
		</layer>
		<layer id="31" name="shadow/BatchNorm_2/FusedBatchNorm/mean/Fused_Mul__copy2181625/restored_convert/quantized20452715" type="Const" version="opset1">
			<data element_type="i8" shape="128, 64, 3, 3" offset="1268" size="73728" />
			<output>
				<port id="0" precision="I8">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="32" name="shadow/BatchNorm_2/FusedBatchNorm/mean/Fused_Mul__copy2181625/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="33" name="shadow/Conv_1/Conv2D/fq_weights_1/zero_point20642913" type="Const" version="opset1">
			<data element_type="f32" shape="128, 1, 1, 1" offset="74996" size="512" />
			<output>
				<port id="0" precision="FP32">
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="34" name="shadow/Conv_1/Conv2D/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="35" name="shadow/Conv_1/Conv2D/fq_weights_1/scale20532886" type="Const" version="opset1">
			<data element_type="f32" shape="128, 1, 1, 1" offset="75508" size="512" />
			<output>
				<port id="0" precision="FP32">
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="36" name="shadow/Conv_1/Conv2D/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="37" name="shadow/Conv_1/Conv2D" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="same_upper" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/Conv_1/Conv2D" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>128</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
			</output>
		</layer>
		<layer id="38" name="data_add_534453492212883" type="Const" version="opset1">
			<data element_type="f16" shape="1, 128, 1, 1" offset="76020" size="256" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="39" name="data_add_534453492211626/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_53445349" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="40" name="shadow/BatchNorm_2/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_2/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm_2/FusedBatchNorm:0">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
			</output>
		</layer>
		<layer id="41" name="shadow/BatchNorm_2/Relu" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_2/Relu" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/BatchNorm_2/Relu:0">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
			</output>
		</layer>
		<layer id="42" name="100110052988" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="43" name="100210062727" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="76276" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="44" name="100310072646" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="45" name="100410082934" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="76276" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="46" name="shadow/MaxPool2D_1/MaxPool/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
			</output>
		</layer>
		<layer id="47" name="shadow/MaxPool2D_1/MaxPool" type="MaxPool" version="opset8">
			<data strides="2, 2" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" kernel="2, 2" rounding_type="floor" auto_pad="valid" index_element_type="i64" axis="0" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/MaxPool2D_1/MaxPool" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>16</dim>
					<dim>60</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/Dropout_2/Identity:0,shadow/MaxPool2D_1/MaxPool:0">
					<dim>1</dim>
					<dim>128</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
					<dim>128</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="48" name="shadow/BatchNorm_3/FusedBatchNorm/mean/Fused_Mul__copy2261627/restored_convert/quantized18952658" type="Const" version="opset1">
			<data element_type="i8" shape="256, 128, 3, 3" offset="76280" size="294912" />
			<output>
				<port id="0" precision="I8">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="49" name="shadow/BatchNorm_3/FusedBatchNorm/mean/Fused_Mul__copy2261627/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="50" name="shadow/Conv_2/Conv2D/fq_weights_1/zero_point19142748" type="Const" version="opset1">
			<data element_type="f32" shape="256, 1, 1, 1" offset="371192" size="1024" />
			<output>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="51" name="shadow/Conv_2/Conv2D/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="52" name="shadow/Conv_2/Conv2D/fq_weights_1/scale19032769" type="Const" version="opset1">
			<data element_type="f32" shape="256, 1, 1, 1" offset="372216" size="1024" />
			<output>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="53" name="shadow/Conv_2/Conv2D/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="54" name="shadow/Conv_2/Conv2D" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="same_upper" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/Conv_2/Conv2D" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>128</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>128</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="55" name="data_add_535253572292817" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256, 1, 1" offset="373240" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="56" name="data_add_535253572291628/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_53525357" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="57" name="shadow/BatchNorm_3/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_3/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm_3/FusedBatchNorm:0">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="58" name="shadow/BatchNorm_3/Relu" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_3/Relu" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/BatchNorm_3/Relu:0,shadow/Dropout_3/Identity:0">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="59" name="7957992724" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="60" name="7968002991" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="373752" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="61" name="7978012694" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="62" name="7988022637" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="373752" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="63" name="shadow/Conv_3/Conv2D/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="64" name="shadow/BatchNorm_4/FusedBatchNorm/mean/Fused_Mul__copy2331629/restored_convert/quantized18652961" type="Const" version="opset1">
			<data element_type="i8" shape="256, 256, 3, 3" offset="373756" size="589824" />
			<output>
				<port id="0" precision="I8">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="65" name="shadow/BatchNorm_4/FusedBatchNorm/mean/Fused_Mul__copy2331629/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="66" name="shadow/Conv_3/Conv2D/fq_weights_1/zero_point18842997" type="Const" version="opset1">
			<data element_type="f32" shape="256, 1, 1, 1" offset="963580" size="1024" />
			<output>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="67" name="shadow/Conv_3/Conv2D/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="68" name="shadow/Conv_3/Conv2D/fq_weights_1/scale18732664" type="Const" version="opset1">
			<data element_type="f32" shape="256, 1, 1, 1" offset="964604" size="1024" />
			<output>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="69" name="shadow/Conv_3/Conv2D/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="70" name="shadow/Conv_3/Conv2D" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="same_upper" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/Conv_3/Conv2D" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>256</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="71" name="data_add_536053652362634" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256, 1, 1" offset="965628" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="72" name="data_add_536053652361630/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_53605365" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="73" name="shadow/BatchNorm_4/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_4/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm_4/FusedBatchNorm:0">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="74" name="shadow/BatchNorm_4/Relu" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_4/Relu" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/BatchNorm_4/Relu:0">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="75" name="101110152892" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="76" name="101210162994" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="966140" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="77" name="101310172721" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="78" name="101410182742" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="966140" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="79" name="shadow/MaxPool2D_2/MaxPool/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="80" name="shadow/MaxPool2D_2/MaxPool" type="MaxPool" version="opset8">
			<data strides="2, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" kernel="2, 1" rounding_type="floor" auto_pad="valid" index_element_type="i64" axis="0" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/MaxPool2D_2/MaxPool" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>8</dim>
					<dim>30</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/Dropout_4/Identity:0,shadow/MaxPool2D_2/MaxPool:0">
					<dim>1</dim>
					<dim>256</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
					<dim>256</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="81" name="shadow/BatchNorm_5/FusedBatchNorm/mean/Fused_Mul__copy2411631/restored_convert/quantized19552925" type="Const" version="opset1">
			<data element_type="i8" shape="512, 256, 3, 3" offset="966144" size="1179648" />
			<output>
				<port id="0" precision="I8">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="82" name="shadow/BatchNorm_5/FusedBatchNorm/mean/Fused_Mul__copy2411631/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="83" name="shadow/Conv_4/Conv2D/fq_weights_1/zero_point19742691" type="Const" version="opset1">
			<data element_type="f32" shape="512, 1, 1, 1" offset="2145792" size="2048" />
			<output>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="84" name="shadow/Conv_4/Conv2D/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="85" name="shadow/Conv_4/Conv2D/fq_weights_1/scale19632928" type="Const" version="opset1">
			<data element_type="f32" shape="512, 1, 1, 1" offset="2147840" size="2048" />
			<output>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="86" name="shadow/Conv_4/Conv2D/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="87" name="shadow/Conv_4/Conv2D" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="same_upper" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/Conv_4/Conv2D" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>256</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="88" name="data_add_536853732442655" type="Const" version="opset1">
			<data element_type="f16" shape="1, 512, 1, 1" offset="2149888" size="1024" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="89" name="data_add_536853732441632/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_53685373" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="90" name="shadow/BatchNorm_5/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_5/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm_5/FusedBatchNorm:0">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="91" name="shadow/BatchNorm_5/Relu" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_5/Relu" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/BatchNorm_5/Relu:0,shadow/Dropout_5/Identity:0">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="92" name="7557592862" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="93" name="7567602787" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="2150912" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="94" name="7577612688" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="95" name="7587622811" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="2150912" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="96" name="shadow/Conv_5/Conv2D/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="97" name="shadow/BatchNorm_6/FusedBatchNorm/mean/Fused_Mul__copy2481633/restored_convert/quantized20152937" type="Const" version="opset1">
			<data element_type="i8" shape="512, 512, 3, 3" offset="2150916" size="2359296" />
			<output>
				<port id="0" precision="I8">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="98" name="shadow/BatchNorm_6/FusedBatchNorm/mean/Fused_Mul__copy2481633/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="99" name="shadow/Conv_5/Conv2D/fq_weights_1/zero_point20342736" type="Const" version="opset1">
			<data element_type="f32" shape="512, 1, 1, 1" offset="4510212" size="2048" />
			<output>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="100" name="shadow/Conv_5/Conv2D/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="101" name="shadow/Conv_5/Conv2D/fq_weights_1/scale20232709" type="Const" version="opset1">
			<data element_type="f32" shape="512, 1, 1, 1" offset="4512260" size="2048" />
			<output>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="102" name="shadow/Conv_5/Conv2D/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="103" name="shadow/Conv_5/Conv2D" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="same_upper" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/Conv_5/Conv2D" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="104" name="data_add_537653812512649" type="Const" version="opset1">
			<data element_type="f16" shape="1, 512, 1, 1" offset="4514308" size="1024" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="105" name="data_add_537653812511634/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_53765381" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="106" name="shadow/BatchNorm_6/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_6/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm_6/FusedBatchNorm:0">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="107" name="shadow/BatchNorm_6/Relu" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_6/Relu" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/BatchNorm_6/Relu:0">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="108" name="102110252844" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="109" name="102210263003" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="4515332" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="110" name="102310272847" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="1260" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="111" name="102410282895" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="4515332" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="112" name="shadow/MaxPool2D_3/MaxPool/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="113" name="shadow/MaxPool2D_3/MaxPool" type="MaxPool" version="opset8">
			<data strides="2, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" kernel="2, 1" rounding_type="floor" auto_pad="valid" index_element_type="i64" axis="0" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/MaxPool2D_3/MaxPool" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>4</dim>
					<dim>30</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/Dropout_6/Identity:0,shadow/MaxPool2D_3/MaxPool:0">
					<dim>1</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>30</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="114" name="shadow/BatchNorm_7/FusedBatchNorm/mean/Fused_Mul__copy2561635/restored_convert/quantized18352739" type="Const" version="opset1">
			<data element_type="i8" shape="512, 512, 2, 2" offset="4515336" size="1048576" />
			<output>
				<port id="0" precision="I8">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="115" name="shadow/BatchNorm_7/FusedBatchNorm/mean/Fused_Mul__copy2561635/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="116" name="shadow/Conv_6/Conv2D/fq_weights_1/zero_point18542904" type="Const" version="opset1">
			<data element_type="f32" shape="512, 1, 1, 1" offset="5563912" size="2048" />
			<output>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="117" name="shadow/Conv_6/Conv2D/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="118" name="shadow/Conv_6/Conv2D/fq_weights_1/scale18432760" type="Const" version="opset1">
			<data element_type="f32" shape="512, 1, 1, 1" offset="5565960" size="2048" />
			<output>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="119" name="shadow/Conv_6/Conv2D/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="120" name="shadow/Conv_6/Conv2D" type="Convolution" version="opset1">
			<data strides="2, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="same_upper" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/Conv_6/Conv2D" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>512</dim>
					<dim>512</dim>
					<dim>2</dim>
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="121" name="data_add_538453892592940" type="Const" version="opset1">
			<data element_type="f16" shape="1, 512, 1, 1" offset="5568008" size="1024" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="122" name="data_add_538453892591636/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
				<attribute name="fused_names" version="0" value="data_add_53845389" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="123" name="shadow/BatchNorm_7/FusedBatchNorm/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_7/FusedBatchNorm/variance/Fused_Add_" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/BatchNorm_7/FusedBatchNorm:0">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="124" name="shadow/BatchNorm_7/Relu" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_7/Relu" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>30</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="125" name="Constant_37572622868" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569032" size="8" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_7/Relu/Transpose, shadow/Squeeze" />
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="126" name="Squeeze_3758" type="Squeeze" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_7/Relu/Transpose, shadow/Squeeze" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>1</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>30</dim>
				</port>
			</output>
		</layer>
		<layer id="127" name="Constant_3759264" type="Const" version="opset1">
			<data element_type="i64" shape="3" offset="5569040" size="24" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_7/Relu/Transpose, shadow/Squeeze" />
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="128" name="shadow/Squeeze" type="Transpose" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/BatchNorm_7/Relu/Transpose, shadow/Squeeze" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>512</dim>
					<dim>30</dim>
				</port>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/ReverseV2:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/transpose:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/transpose:0,shadow/Squeeze:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="129" name="Constant_13392672643" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="130" name="Constant_13392671639/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="131" name="ShapeOf_1335" type="ShapeOf" version="opset3">
			<data output_type="i64" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="132" name="Constant_1337269" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="133" name="Constant_1336270" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="134" name="Gather_1338" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="135" name="Constant_13452722916" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="136" name="Concat_1346" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="137" name="Broadcast_1347" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="138" name="Constant_13992762784" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="139" name="Constant_13992761643/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="140" name="Constant_14052772919" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="141" name="Concat_1406" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="142" name="Broadcast_1407" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="143" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="0" end="-1" stride="1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="0" end="-1" stride="1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253642151" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25364" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352_compressed2360" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="5569600" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311355_compressed2548" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="6618176" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311355" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311355" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter_compressed2757" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="7142464" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283583154" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28358" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="144" name="Constant_14672822964" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="145" name="Constant_14672821645/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="146" name="Constant_1465283" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="147" name="Constant_1464284" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="148" name="Gather_1466" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="149" name="Constant_14732862754" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="150" name="Concat_1474" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="151" name="Broadcast_1475" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="152" name="Constant_15272902952" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="153" name="Constant_15272901649/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="154" name="Constant_15332912841" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="155" name="Concat_1534" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="156" name="Broadcast_1535" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="157" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="-1" end="0" stride="-1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="-1" end="0" stride="-1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/ReverseV2:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253642148" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25364" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310361_compressed2351" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="7144512" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310361" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310361" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311352_compressed2560" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="8193088" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311352" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311352" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter_compressed2757" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="8717376" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283583154" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28358" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="158" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/concat" type="Concat" version="opset1">
			<data axis="2" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/concat" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_0/concat:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/ReverseV2:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/transpose:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/transpose:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="159" name="Constant_15952972766" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="160" name="Constant_15952971651/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="161" name="ShapeOf_1591" type="ShapeOf" version="opset3">
			<data output_type="i64" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="162" name="Constant_1593299" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="163" name="Constant_1592300" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="164" name="Gather_1594" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="165" name="Constant_16013022772" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="166" name="Concat_1602" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="167" name="Broadcast_1603" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="168" name="Constant_16243062826" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="169" name="Constant_16243061655/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="170" name="Constant_16303072778" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="171" name="Concat_1631" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="172" name="Broadcast_1632" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="173" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="0" end="-1" stride="1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="0" end="-1" stride="1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253612157" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25361" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310364_compressed2348" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="8719424" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310364" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310364" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311358_compressed2554" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="9768000" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311358" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311358" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter_compressed2760" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="10292288" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283523151" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28352" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="174" name="Constant_16613122856" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="175" name="Constant_16613121657/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="176" name="Constant_1659313" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="177" name="Constant_1658314" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="178" name="Gather_1660" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="179" name="Constant_16673162706" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="180" name="Concat_1668" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="181" name="Broadcast_1669" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="182" name="Constant_16903202790" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="183" name="Constant_16903201661/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="184" name="Constant_16963212982" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="185" name="Concat_1697" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="186" name="Broadcast_1698" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="187" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="-1" end="0" stride="-1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="-1" end="0" stride="-1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/ReverseV2:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253642151" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25364" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310352_compressed2348" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="10294336" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310352" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310352" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311361_compressed2560" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="11342912" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311361" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311361" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter_compressed2757" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="11867200" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283583154" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28358" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="188" name="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/concat" type="Concat" version="opset1">
			<data axis="2" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/concat" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/ReverseV2:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/transpose:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/transpose:0,shadow/LSTMLayers/encoder/dropout/Identity:0,shadow/LSTMLayers/encoder/stack_bidirectional_rnn/cell_1/concat:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="189" name="Constant_17273272973" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="190" name="Constant_17273271663/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="191" name="ShapeOf_1723" type="ShapeOf" version="opset3">
			<data output_type="i64" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="192" name="Constant_1725329" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="193" name="Constant_1724330" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="194" name="Gather_1726" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="195" name="Constant_17333322799" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="196" name="Concat_1734" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="197" name="Broadcast_1735" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="198" name="Constant_17563362733" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="199" name="Constant_17563361667/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="200" name="Constant_17623372667" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="201" name="Concat_1763" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="202" name="Broadcast_1764" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="203" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="0" end="-1" stride="1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="0" end="-1" stride="1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253642148" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25364" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352_compressed2357" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="11869248" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311355_compressed2560" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="12917824" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311355" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311355" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter_compressed2754" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="13442112" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283613151" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28361" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="204" name="Constant_17933423000" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="205" name="Constant_17933421669/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="206" name="Constant_1791343" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="207" name="Constant_1790344" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="208" name="Gather_1792" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="209" name="Constant_17993462853" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="210" name="Concat_1800" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="211" name="Broadcast_1801" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="212" name="Constant_18223502670" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="213" name="Constant_18223501673/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="214" name="Constant_18283512775" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="215" name="Concat_1829" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="216" name="Broadcast_1830" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="217" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="-1" end="0" stride="-1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="-1" end="0" stride="-1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/ReverseV2:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253612154" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25361" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310364_compressed2348" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="13444160" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310364" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310364" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311352_compressed2551" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="14492736" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311352" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311352" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter_compressed2760" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="15017024" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283583157" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28358" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="218" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/concat" type="Concat" version="opset1">
			<data axis="2" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/concat" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_0/concat:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/ReverseV2:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/transpose:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/transpose:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="219" name="Constant_18593572931" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="220" name="Constant_18593571675/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="221" name="ShapeOf_1855" type="ShapeOf" version="opset3">
			<data output_type="i64" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="222" name="Constant_1857359" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="223" name="Constant_1856360" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="224" name="Gather_1858" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="225" name="Constant_18653622757" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="226" name="Concat_1866" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="227" name="Broadcast_1867" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="228" name="Constant_18883662712" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="229" name="Constant_18883661679/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="230" name="Constant_18943672805" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="231" name="Concat_1895" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="232" name="Broadcast_1896" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="233" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="0" end="-1" stride="1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="0" end="-1" stride="1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253642151" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25364" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352_compressed2360" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="15019072" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split310352" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311361_compressed2554" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="16067648" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311361" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Split311361" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter_compressed2748" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="16591936" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283583157" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28358" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="234" name="104110452673" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="16593984" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="235" name="104210462976" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="16593988" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="236" name="104310472859" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="16593984" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="237" name="104410482730" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="16593988" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="238" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/concat/fq_input_0" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="239" name="Constant_19253722808" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="240" name="Constant_19253721681/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Enter_4:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="241" name="Constant_1923373" type="Const" version="opset1">
			<data element_type="i32" shape="1" offset="1260" size="4" />
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="242" name="Constant_1922374" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="5569576" size="8" />
			<output>
				<port id="0" precision="I64" />
			</output>
		</layer>
		<layer id="243" name="Gather_1924" type="Gather" version="opset8">
			<data batch_dims="0" />
			<input>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
				<port id="1" precision="I32">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64" />
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="244" name="Constant_19313762985" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="245" name="Concat_1932" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="246" name="Broadcast_1933" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="247" name="Constant_19543802820" type="Const" version="opset1">
			<data element_type="f16" shape="1, 256" offset="5569064" size="512" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="248" name="Constant_19543801685/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Enter_3:0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="249" name="Constant_19603812910" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="5569584" size="8" />
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="250" name="Concat_1961" type="Concat" version="opset1">
			<data axis="0" />
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="251" name="Broadcast_1962" type="Broadcast" version="opset3">
			<data mode="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="252" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/LoopCond/TensorIteratorCondition_/TensorIterator" type="TensorIterator" version="opset1">
			<port_map>
				<input axis="1" external_port_id="0" internal_layer_id="2" start="-1" end="0" stride="-1" part_size="1" />
				<input external_port_id="1" internal_layer_id="1" />
				<input external_port_id="2" internal_layer_id="0" />
				<output axis="1" external_port_id="3" internal_layer_id="14" start="-1" end="0" stride="-1" part_size="1" />
			</port_map>
			<back_edges>
				<edge from-layer="15" to-layer="1" />
				<edge from-layer="16" to-layer="0" />
			</back_edges>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/ReverseV2:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3:0,shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/transpose_1:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
			<body>
				<layers>
					<layer id="2" name="32" type="Parameter" version="opset1">
						<data shape="1,1,512" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="32" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="1" name="34" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="34" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="0" name="36" type="Parameter" version="opset1">
						<data shape="1,256" element_type="f32" />
						<rt_info>
							<attribute name="fused_names" version="0" value="36" />
							<attribute name="old_api_map_element_type" version="0" value="f16" />
						</rt_info>
						<output>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="3" name="253612154" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="25361" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="4" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" type="Squeeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Output_0/Data_/InputSqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3:0">
								<dim>1</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="5" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310355_compressed2360" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 512" offset="16593992" size="1048576" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="6" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310355" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split310355" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
						</output>
					</layer>
					<layer id="7" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311358_compressed2557" type="Const" version="opset1">
						<data element_type="f16" shape="1024, 256" offset="17642568" size="524288" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="8" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311358" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Split311358" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="9" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter_compressed2751" type="Const" version="opset1">
						<data element_type="f16" shape="1024" offset="18166856" size="2048" />
						<output>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="10" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" type="Convert" version="opset1">
						<data destination_type="f32" />
						<rt_info>
							<attribute name="decompression" version="0" />
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter" />
						</rt_info>
						<input>
							<port id="0" precision="FP16">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="1" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter:0">
								<dim>1024</dim>
							</port>
						</output>
					</layer>
					<layer id="11" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" type="LSTMCell" version="opset4">
						<data hidden_size="256" activations="sigmoid, tanh, tanh" activations_alpha="" activations_beta="" clip="0" />
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>512</dim>
							</port>
							<port id="1" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="3" precision="FP32">
								<dim>1024</dim>
								<dim>512</dim>
							</port>
							<port id="4" precision="FP32">
								<dim>1024</dim>
								<dim>256</dim>
							</port>
							<port id="5" precision="FP32">
								<dim>1024</dim>
							</port>
						</input>
						<output>
							<port id="6" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="7" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/add_1:0">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="12" name="283523148" type="Const" version="opset1">
						<data element_type="i64" shape="1" offset="5569592" size="8" />
						<rt_info>
							<attribute name="fused_names" version="0" value="28352" />
						</rt_info>
						<output>
							<port id="0" precision="I64">
								<dim>1</dim>
							</port>
						</output>
					</layer>
					<layer id="13" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" type="Unsqueeze" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/OutputUnsqueeze" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
							<port id="1" precision="I64">
								<dim>1</dim>
							</port>
						</input>
						<output>
							<port id="2" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</output>
					</layer>
					<layer id="16" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_1/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="15" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/LSTMCell/Output_0/Data_/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
					<layer id="14" name="30/sink_port_0" type="Result" version="opset1">
						<rt_info>
							<attribute name="fused_names" version="0" value="30/sink_port_0" />
						</rt_info>
						<input>
							<port id="0" precision="FP32">
								<dim>1</dim>
								<dim>1</dim>
								<dim>256</dim>
							</port>
						</input>
					</layer>
				</layers>
				<edges>
					<edge from-layer="0" from-port="0" to-layer="11" to-port="2" />
					<edge from-layer="1" from-port="0" to-layer="11" to-port="1" />
					<edge from-layer="2" from-port="0" to-layer="4" to-port="0" />
					<edge from-layer="3" from-port="0" to-layer="4" to-port="1" />
					<edge from-layer="4" from-port="2" to-layer="11" to-port="0" />
					<edge from-layer="5" from-port="0" to-layer="6" to-port="0" />
					<edge from-layer="6" from-port="1" to-layer="11" to-port="3" />
					<edge from-layer="7" from-port="0" to-layer="8" to-port="0" />
					<edge from-layer="8" from-port="1" to-layer="11" to-port="4" />
					<edge from-layer="9" from-port="0" to-layer="10" to-port="0" />
					<edge from-layer="10" from-port="1" to-layer="11" to-port="5" />
					<edge from-layer="11" from-port="6" to-layer="13" to-port="0" />
					<edge from-layer="11" from-port="6" to-layer="15" to-port="0" />
					<edge from-layer="11" from-port="7" to-layer="16" to-port="0" />
					<edge from-layer="12" from-port="0" to-layer="13" to-port="1" />
					<edge from-layer="13" from-port="2" to-layer="14" to-port="0" />
				</edges>
			</body>
		</layer>
		<layer id="253" name="105110552652" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="18168904" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="254" name="105210562922" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="18168908" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="255" name="105310572661" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="18168904" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="256" name="105410582898" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="18168908" size="4" />
			<output>
				<port id="0" precision="FP32" />
			</output>
		</layer>
		<layer id="257" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/concat/fq_input_1" type="FakeQuantize" version="opset1">
			<data levels="256" auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="FP32" />
				<port id="2" precision="FP32" />
				<port id="3" precision="FP32" />
				<port id="4" precision="FP32" />
			</input>
			<output>
				<port id="5" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="258" name="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/concat" type="Concat" version="opset1">
			<data axis="2" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/concat" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/decoder/stack_bidirectional_rnn/cell_1/concat:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="259" name="Constant_28573862943" type="Const" version="opset1">
			<data element_type="i64" shape="3" offset="18168912" size="24" />
			<output>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="260" name="shadow/LSTMLayers/Reshape" type="Reshape" version="opset1">
			<data special_zero="true" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/Reshape" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/Reshape:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="261" name="shadow/LSTMLayers/fully_connected/weights/read3891688/restored_convert/quantized19252802" type="Const" version="opset1">
			<data element_type="i8" shape="37, 512" offset="18168936" size="18944" />
			<output>
				<port id="0" precision="I8">
					<dim>37</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="262" name="shadow/LSTMLayers/fully_connected/weights/read3891688/restored_convert/quantized/to_f32" type="Convert" version="opset1">
			<data destination_type="f32" />
			<input>
				<port id="0" precision="I8">
					<dim>37</dim>
					<dim>512</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>37</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="263" name="shadow/LSTMLayers/fully_connected/MatMul/fq_weights_1/zero_point19442676" type="Const" version="opset1">
			<data element_type="f32" shape="37, 1" offset="18187880" size="148" />
			<output>
				<port id="0" precision="FP32">
					<dim>37</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="264" name="shadow/LSTMLayers/fully_connected/MatMul/fq_weights_1/minus_zp" type="Subtract" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>37</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>37</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>37</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="265" name="shadow/LSTMLayers/fully_connected/MatMul/fq_weights_1/scale19332682" type="Const" version="opset1">
			<data element_type="f32" shape="37, 1" offset="18188028" size="148" />
			<output>
				<port id="0" precision="FP32">
					<dim>37</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="266" name="shadow/LSTMLayers/fully_connected/MatMul/fq_weights_1/mulpiply_by_scale" type="Multiply" version="opset1">
			<data auto_broadcast="numpy" />
			<input>
				<port id="0" precision="FP32">
					<dim>37</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>37</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>37</dim>
					<dim>512</dim>
				</port>
			</output>
		</layer>
		<layer id="267" name="shadow/LSTMLayers/fully_connected/MatMul" type="MatMul" version="opset1">
			<data transpose_a="false" transpose_b="true" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/fully_connected/MatMul" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>512</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>37</dim>
					<dim>512</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/fully_connected/MatMul:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>37</dim>
				</port>
			</output>
		</layer>
		<layer id="268" name="Constant_38053922907" type="Const" version="opset1">
			<data element_type="f16" shape="1, 1, 37" offset="18188176" size="74" />
			<output>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>1</dim>
					<dim>37</dim>
				</port>
			</output>
		</layer>
		<layer id="269" name="Constant_38053921689/restored_convert" type="Convert" version="opset1">
			<data destination_type="f32" />
			<rt_info>
				<attribute name="decompression" version="0" />
			</rt_info>
			<input>
				<port id="0" precision="FP16">
					<dim>1</dim>
					<dim>1</dim>
					<dim>37</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>37</dim>
				</port>
			</output>
		</layer>
		<layer id="270" name="shadow/LSTMLayers/fully_connected/BiasAdd/Add" type="Add" version="opset1">
			<data auto_broadcast="numpy" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/fully_connected/BiasAdd/Add" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>37</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>37</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/fully_connected/BiasAdd:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>37</dim>
				</port>
			</output>
		</layer>
		<layer id="271" name="Constant_28583942640" type="Const" version="opset1">
			<data element_type="i64" shape="3" offset="18188250" size="24" />
			<output>
				<port id="0" precision="I64">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="272" name="shadow/LSTMLayers/Reshape_1" type="Reshape" version="opset1">
			<data special_zero="true" />
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/Reshape_1" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>37</dim>
				</port>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="shadow/LSTMLayers/Reshape_1:0,shadow/LSTMLayers/transpose_time_major,shadow/LSTMLayers/transpose_time_major:0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>37</dim>
				</port>
			</output>
		</layer>
		<layer id="273" name="shadow/LSTMLayers/transpose_time_major:0" type="Result" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="shadow/LSTMLayers/transpose_time_major:0" />
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>37</dim>
				</port>
			</input>
		</layer>
	</layers>
	<edges>
		<edge from-layer="0" from-port="0" to-layer="2" to-port="0" />
		<edge from-layer="1" from-port="0" to-layer="2" to-port="1" />
		<edge from-layer="2" from-port="2" to-layer="5" to-port="0" />
		<edge from-layer="3" from-port="0" to-layer="4" to-port="0" />
		<edge from-layer="4" from-port="1" to-layer="5" to-port="1" />
		<edge from-layer="5" from-port="2" to-layer="8" to-port="0" />
		<edge from-layer="6" from-port="0" to-layer="7" to-port="0" />
		<edge from-layer="7" from-port="1" to-layer="8" to-port="1" />
		<edge from-layer="8" from-port="2" to-layer="13" to-port="0" />
		<edge from-layer="9" from-port="0" to-layer="13" to-port="1" />
		<edge from-layer="10" from-port="0" to-layer="13" to-port="2" />
		<edge from-layer="11" from-port="0" to-layer="13" to-port="3" />
		<edge from-layer="12" from-port="0" to-layer="13" to-port="4" />
		<edge from-layer="13" from-port="5" to-layer="20" to-port="0" />
		<edge from-layer="14" from-port="0" to-layer="15" to-port="0" />
		<edge from-layer="15" from-port="1" to-layer="17" to-port="0" />
		<edge from-layer="16" from-port="0" to-layer="17" to-port="1" />
		<edge from-layer="17" from-port="2" to-layer="19" to-port="0" />
		<edge from-layer="18" from-port="0" to-layer="19" to-port="1" />
		<edge from-layer="19" from-port="2" to-layer="20" to-port="1" />
		<edge from-layer="20" from-port="2" to-layer="23" to-port="0" />
		<edge from-layer="21" from-port="0" to-layer="22" to-port="0" />
		<edge from-layer="22" from-port="1" to-layer="23" to-port="1" />
		<edge from-layer="23" from-port="2" to-layer="24" to-port="0" />
		<edge from-layer="24" from-port="1" to-layer="29" to-port="0" />
		<edge from-layer="25" from-port="0" to-layer="29" to-port="1" />
		<edge from-layer="26" from-port="0" to-layer="29" to-port="2" />
		<edge from-layer="27" from-port="0" to-layer="29" to-port="3" />
		<edge from-layer="28" from-port="0" to-layer="29" to-port="4" />
		<edge from-layer="29" from-port="5" to-layer="30" to-port="0" />
		<edge from-layer="30" from-port="1" to-layer="37" to-port="0" />
		<edge from-layer="31" from-port="0" to-layer="32" to-port="0" />
		<edge from-layer="32" from-port="1" to-layer="34" to-port="0" />
		<edge from-layer="33" from-port="0" to-layer="34" to-port="1" />
		<edge from-layer="34" from-port="2" to-layer="36" to-port="0" />
		<edge from-layer="35" from-port="0" to-layer="36" to-port="1" />
		<edge from-layer="36" from-port="2" to-layer="37" to-port="1" />
		<edge from-layer="37" from-port="2" to-layer="40" to-port="0" />
		<edge from-layer="38" from-port="0" to-layer="39" to-port="0" />
		<edge from-layer="39" from-port="1" to-layer="40" to-port="1" />
		<edge from-layer="40" from-port="2" to-layer="41" to-port="0" />
		<edge from-layer="41" from-port="1" to-layer="46" to-port="0" />
		<edge from-layer="42" from-port="0" to-layer="46" to-port="1" />
		<edge from-layer="43" from-port="0" to-layer="46" to-port="2" />
		<edge from-layer="44" from-port="0" to-layer="46" to-port="3" />
		<edge from-layer="45" from-port="0" to-layer="46" to-port="4" />
		<edge from-layer="46" from-port="5" to-layer="47" to-port="0" />
		<edge from-layer="47" from-port="1" to-layer="54" to-port="0" />
		<edge from-layer="48" from-port="0" to-layer="49" to-port="0" />
		<edge from-layer="49" from-port="1" to-layer="51" to-port="0" />
		<edge from-layer="50" from-port="0" to-layer="51" to-port="1" />
		<edge from-layer="51" from-port="2" to-layer="53" to-port="0" />
		<edge from-layer="52" from-port="0" to-layer="53" to-port="1" />
		<edge from-layer="53" from-port="2" to-layer="54" to-port="1" />
		<edge from-layer="54" from-port="2" to-layer="57" to-port="0" />
		<edge from-layer="55" from-port="0" to-layer="56" to-port="0" />
		<edge from-layer="56" from-port="1" to-layer="57" to-port="1" />
		<edge from-layer="57" from-port="2" to-layer="58" to-port="0" />
		<edge from-layer="58" from-port="1" to-layer="63" to-port="0" />
		<edge from-layer="59" from-port="0" to-layer="63" to-port="1" />
		<edge from-layer="60" from-port="0" to-layer="63" to-port="2" />
		<edge from-layer="61" from-port="0" to-layer="63" to-port="3" />
		<edge from-layer="62" from-port="0" to-layer="63" to-port="4" />
		<edge from-layer="63" from-port="5" to-layer="70" to-port="0" />
		<edge from-layer="64" from-port="0" to-layer="65" to-port="0" />
		<edge from-layer="65" from-port="1" to-layer="67" to-port="0" />
		<edge from-layer="66" from-port="0" to-layer="67" to-port="1" />
		<edge from-layer="67" from-port="2" to-layer="69" to-port="0" />
		<edge from-layer="68" from-port="0" to-layer="69" to-port="1" />
		<edge from-layer="69" from-port="2" to-layer="70" to-port="1" />
		<edge from-layer="70" from-port="2" to-layer="73" to-port="0" />
		<edge from-layer="71" from-port="0" to-layer="72" to-port="0" />
		<edge from-layer="72" from-port="1" to-layer="73" to-port="1" />
		<edge from-layer="73" from-port="2" to-layer="74" to-port="0" />
		<edge from-layer="74" from-port="1" to-layer="79" to-port="0" />
		<edge from-layer="75" from-port="0" to-layer="79" to-port="1" />
		<edge from-layer="76" from-port="0" to-layer="79" to-port="2" />
		<edge from-layer="77" from-port="0" to-layer="79" to-port="3" />
		<edge from-layer="78" from-port="0" to-layer="79" to-port="4" />
		<edge from-layer="79" from-port="5" to-layer="80" to-port="0" />
		<edge from-layer="80" from-port="1" to-layer="87" to-port="0" />
		<edge from-layer="81" from-port="0" to-layer="82" to-port="0" />
		<edge from-layer="82" from-port="1" to-layer="84" to-port="0" />
		<edge from-layer="83" from-port="0" to-layer="84" to-port="1" />
		<edge from-layer="84" from-port="2" to-layer="86" to-port="0" />
		<edge from-layer="85" from-port="0" to-layer="86" to-port="1" />
		<edge from-layer="86" from-port="2" to-layer="87" to-port="1" />
		<edge from-layer="87" from-port="2" to-layer="90" to-port="0" />
		<edge from-layer="88" from-port="0" to-layer="89" to-port="0" />
		<edge from-layer="89" from-port="1" to-layer="90" to-port="1" />
		<edge from-layer="90" from-port="2" to-layer="91" to-port="0" />
		<edge from-layer="91" from-port="1" to-layer="96" to-port="0" />
		<edge from-layer="92" from-port="0" to-layer="96" to-port="1" />
		<edge from-layer="93" from-port="0" to-layer="96" to-port="2" />
		<edge from-layer="94" from-port="0" to-layer="96" to-port="3" />
		<edge from-layer="95" from-port="0" to-layer="96" to-port="4" />
		<edge from-layer="96" from-port="5" to-layer="103" to-port="0" />
		<edge from-layer="97" from-port="0" to-layer="98" to-port="0" />
		<edge from-layer="98" from-port="1" to-layer="100" to-port="0" />
		<edge from-layer="99" from-port="0" to-layer="100" to-port="1" />
		<edge from-layer="100" from-port="2" to-layer="102" to-port="0" />
		<edge from-layer="101" from-port="0" to-layer="102" to-port="1" />
		<edge from-layer="102" from-port="2" to-layer="103" to-port="1" />
		<edge from-layer="103" from-port="2" to-layer="106" to-port="0" />
		<edge from-layer="104" from-port="0" to-layer="105" to-port="0" />
		<edge from-layer="105" from-port="1" to-layer="106" to-port="1" />
		<edge from-layer="106" from-port="2" to-layer="107" to-port="0" />
		<edge from-layer="107" from-port="1" to-layer="112" to-port="0" />
		<edge from-layer="108" from-port="0" to-layer="112" to-port="1" />
		<edge from-layer="109" from-port="0" to-layer="112" to-port="2" />
		<edge from-layer="110" from-port="0" to-layer="112" to-port="3" />
		<edge from-layer="111" from-port="0" to-layer="112" to-port="4" />
		<edge from-layer="112" from-port="5" to-layer="113" to-port="0" />
		<edge from-layer="113" from-port="1" to-layer="120" to-port="0" />
		<edge from-layer="114" from-port="0" to-layer="115" to-port="0" />
		<edge from-layer="115" from-port="1" to-layer="117" to-port="0" />
		<edge from-layer="116" from-port="0" to-layer="117" to-port="1" />
		<edge from-layer="117" from-port="2" to-layer="119" to-port="0" />
		<edge from-layer="118" from-port="0" to-layer="119" to-port="1" />
		<edge from-layer="119" from-port="2" to-layer="120" to-port="1" />
		<edge from-layer="120" from-port="2" to-layer="123" to-port="0" />
		<edge from-layer="121" from-port="0" to-layer="122" to-port="0" />
		<edge from-layer="122" from-port="1" to-layer="123" to-port="1" />
		<edge from-layer="123" from-port="2" to-layer="124" to-port="0" />
		<edge from-layer="124" from-port="1" to-layer="126" to-port="0" />
		<edge from-layer="125" from-port="0" to-layer="126" to-port="1" />
		<edge from-layer="126" from-port="2" to-layer="128" to-port="0" />
		<edge from-layer="127" from-port="0" to-layer="128" to-port="1" />
		<edge from-layer="128" from-port="2" to-layer="131" to-port="0" />
		<edge from-layer="128" from-port="2" to-layer="143" to-port="0" />
		<edge from-layer="128" from-port="2" to-layer="157" to-port="0" />
		<edge from-layer="129" from-port="0" to-layer="130" to-port="0" />
		<edge from-layer="130" from-port="1" to-layer="137" to-port="0" />
		<edge from-layer="131" from-port="1" to-layer="134" to-port="0" />
		<edge from-layer="131" from-port="1" to-layer="148" to-port="0" />
		<edge from-layer="132" from-port="0" to-layer="134" to-port="1" />
		<edge from-layer="133" from-port="0" to-layer="134" to-port="2" />
		<edge from-layer="134" from-port="3" to-layer="136" to-port="0" />
		<edge from-layer="134" from-port="3" to-layer="141" to-port="0" />
		<edge from-layer="135" from-port="0" to-layer="136" to-port="1" />
		<edge from-layer="136" from-port="2" to-layer="137" to-port="1" />
		<edge from-layer="137" from-port="2" to-layer="143" to-port="1" />
		<edge from-layer="138" from-port="0" to-layer="139" to-port="0" />
		<edge from-layer="139" from-port="1" to-layer="142" to-port="0" />
		<edge from-layer="140" from-port="0" to-layer="141" to-port="1" />
		<edge from-layer="141" from-port="2" to-layer="142" to-port="1" />
		<edge from-layer="142" from-port="2" to-layer="143" to-port="2" />
		<edge from-layer="143" from-port="3" to-layer="158" to-port="0" />
		<edge from-layer="144" from-port="0" to-layer="145" to-port="0" />
		<edge from-layer="145" from-port="1" to-layer="151" to-port="0" />
		<edge from-layer="146" from-port="0" to-layer="148" to-port="1" />
		<edge from-layer="147" from-port="0" to-layer="148" to-port="2" />
		<edge from-layer="148" from-port="3" to-layer="150" to-port="0" />
		<edge from-layer="148" from-port="3" to-layer="155" to-port="0" />
		<edge from-layer="149" from-port="0" to-layer="150" to-port="1" />
		<edge from-layer="150" from-port="2" to-layer="151" to-port="1" />
		<edge from-layer="151" from-port="2" to-layer="157" to-port="1" />
		<edge from-layer="152" from-port="0" to-layer="153" to-port="0" />
		<edge from-layer="153" from-port="1" to-layer="156" to-port="0" />
		<edge from-layer="154" from-port="0" to-layer="155" to-port="1" />
		<edge from-layer="155" from-port="2" to-layer="156" to-port="1" />
		<edge from-layer="156" from-port="2" to-layer="157" to-port="2" />
		<edge from-layer="157" from-port="3" to-layer="158" to-port="1" />
		<edge from-layer="158" from-port="2" to-layer="161" to-port="0" />
		<edge from-layer="158" from-port="2" to-layer="187" to-port="0" />
		<edge from-layer="158" from-port="2" to-layer="173" to-port="0" />
		<edge from-layer="159" from-port="0" to-layer="160" to-port="0" />
		<edge from-layer="160" from-port="1" to-layer="167" to-port="0" />
		<edge from-layer="161" from-port="1" to-layer="164" to-port="0" />
		<edge from-layer="161" from-port="1" to-layer="178" to-port="0" />
		<edge from-layer="162" from-port="0" to-layer="164" to-port="1" />
		<edge from-layer="163" from-port="0" to-layer="164" to-port="2" />
		<edge from-layer="164" from-port="3" to-layer="166" to-port="0" />
		<edge from-layer="164" from-port="3" to-layer="171" to-port="0" />
		<edge from-layer="165" from-port="0" to-layer="166" to-port="1" />
		<edge from-layer="166" from-port="2" to-layer="167" to-port="1" />
		<edge from-layer="167" from-port="2" to-layer="173" to-port="1" />
		<edge from-layer="168" from-port="0" to-layer="169" to-port="0" />
		<edge from-layer="169" from-port="1" to-layer="172" to-port="0" />
		<edge from-layer="170" from-port="0" to-layer="171" to-port="1" />
		<edge from-layer="171" from-port="2" to-layer="172" to-port="1" />
		<edge from-layer="172" from-port="2" to-layer="173" to-port="2" />
		<edge from-layer="173" from-port="3" to-layer="188" to-port="0" />
		<edge from-layer="174" from-port="0" to-layer="175" to-port="0" />
		<edge from-layer="175" from-port="1" to-layer="181" to-port="0" />
		<edge from-layer="176" from-port="0" to-layer="178" to-port="1" />
		<edge from-layer="177" from-port="0" to-layer="178" to-port="2" />
		<edge from-layer="178" from-port="3" to-layer="180" to-port="0" />
		<edge from-layer="178" from-port="3" to-layer="185" to-port="0" />
		<edge from-layer="179" from-port="0" to-layer="180" to-port="1" />
		<edge from-layer="180" from-port="2" to-layer="181" to-port="1" />
		<edge from-layer="181" from-port="2" to-layer="187" to-port="1" />
		<edge from-layer="182" from-port="0" to-layer="183" to-port="0" />
		<edge from-layer="183" from-port="1" to-layer="186" to-port="0" />
		<edge from-layer="184" from-port="0" to-layer="185" to-port="1" />
		<edge from-layer="185" from-port="2" to-layer="186" to-port="1" />
		<edge from-layer="186" from-port="2" to-layer="187" to-port="2" />
		<edge from-layer="187" from-port="3" to-layer="188" to-port="1" />
		<edge from-layer="188" from-port="2" to-layer="191" to-port="0" />
		<edge from-layer="188" from-port="2" to-layer="217" to-port="0" />
		<edge from-layer="188" from-port="2" to-layer="203" to-port="0" />
		<edge from-layer="189" from-port="0" to-layer="190" to-port="0" />
		<edge from-layer="190" from-port="1" to-layer="197" to-port="0" />
		<edge from-layer="191" from-port="1" to-layer="194" to-port="0" />
		<edge from-layer="191" from-port="1" to-layer="208" to-port="0" />
		<edge from-layer="192" from-port="0" to-layer="194" to-port="1" />
		<edge from-layer="193" from-port="0" to-layer="194" to-port="2" />
		<edge from-layer="194" from-port="3" to-layer="196" to-port="0" />
		<edge from-layer="194" from-port="3" to-layer="201" to-port="0" />
		<edge from-layer="195" from-port="0" to-layer="196" to-port="1" />
		<edge from-layer="196" from-port="2" to-layer="197" to-port="1" />
		<edge from-layer="197" from-port="2" to-layer="203" to-port="1" />
		<edge from-layer="198" from-port="0" to-layer="199" to-port="0" />
		<edge from-layer="199" from-port="1" to-layer="202" to-port="0" />
		<edge from-layer="200" from-port="0" to-layer="201" to-port="1" />
		<edge from-layer="201" from-port="2" to-layer="202" to-port="1" />
		<edge from-layer="202" from-port="2" to-layer="203" to-port="2" />
		<edge from-layer="203" from-port="3" to-layer="218" to-port="0" />
		<edge from-layer="204" from-port="0" to-layer="205" to-port="0" />
		<edge from-layer="205" from-port="1" to-layer="211" to-port="0" />
		<edge from-layer="206" from-port="0" to-layer="208" to-port="1" />
		<edge from-layer="207" from-port="0" to-layer="208" to-port="2" />
		<edge from-layer="208" from-port="3" to-layer="210" to-port="0" />
		<edge from-layer="208" from-port="3" to-layer="215" to-port="0" />
		<edge from-layer="209" from-port="0" to-layer="210" to-port="1" />
		<edge from-layer="210" from-port="2" to-layer="211" to-port="1" />
		<edge from-layer="211" from-port="2" to-layer="217" to-port="1" />
		<edge from-layer="212" from-port="0" to-layer="213" to-port="0" />
		<edge from-layer="213" from-port="1" to-layer="216" to-port="0" />
		<edge from-layer="214" from-port="0" to-layer="215" to-port="1" />
		<edge from-layer="215" from-port="2" to-layer="216" to-port="1" />
		<edge from-layer="216" from-port="2" to-layer="217" to-port="2" />
		<edge from-layer="217" from-port="3" to-layer="218" to-port="1" />
		<edge from-layer="218" from-port="2" to-layer="221" to-port="0" />
		<edge from-layer="218" from-port="2" to-layer="233" to-port="0" />
		<edge from-layer="218" from-port="2" to-layer="252" to-port="0" />
		<edge from-layer="219" from-port="0" to-layer="220" to-port="0" />
		<edge from-layer="220" from-port="1" to-layer="227" to-port="0" />
		<edge from-layer="221" from-port="1" to-layer="224" to-port="0" />
		<edge from-layer="221" from-port="1" to-layer="243" to-port="0" />
		<edge from-layer="222" from-port="0" to-layer="224" to-port="1" />
		<edge from-layer="223" from-port="0" to-layer="224" to-port="2" />
		<edge from-layer="224" from-port="3" to-layer="231" to-port="0" />
		<edge from-layer="224" from-port="3" to-layer="226" to-port="0" />
		<edge from-layer="225" from-port="0" to-layer="226" to-port="1" />
		<edge from-layer="226" from-port="2" to-layer="227" to-port="1" />
		<edge from-layer="227" from-port="2" to-layer="233" to-port="1" />
		<edge from-layer="228" from-port="0" to-layer="229" to-port="0" />
		<edge from-layer="229" from-port="1" to-layer="232" to-port="0" />
		<edge from-layer="230" from-port="0" to-layer="231" to-port="1" />
		<edge from-layer="231" from-port="2" to-layer="232" to-port="1" />
		<edge from-layer="232" from-port="2" to-layer="233" to-port="2" />
		<edge from-layer="233" from-port="3" to-layer="238" to-port="0" />
		<edge from-layer="234" from-port="0" to-layer="238" to-port="1" />
		<edge from-layer="235" from-port="0" to-layer="238" to-port="2" />
		<edge from-layer="236" from-port="0" to-layer="238" to-port="3" />
		<edge from-layer="237" from-port="0" to-layer="238" to-port="4" />
		<edge from-layer="238" from-port="5" to-layer="258" to-port="0" />
		<edge from-layer="239" from-port="0" to-layer="240" to-port="0" />
		<edge from-layer="240" from-port="1" to-layer="246" to-port="0" />
		<edge from-layer="241" from-port="0" to-layer="243" to-port="1" />
		<edge from-layer="242" from-port="0" to-layer="243" to-port="2" />
		<edge from-layer="243" from-port="3" to-layer="245" to-port="0" />
		<edge from-layer="243" from-port="3" to-layer="250" to-port="0" />
		<edge from-layer="244" from-port="0" to-layer="245" to-port="1" />
		<edge from-layer="245" from-port="2" to-layer="246" to-port="1" />
		<edge from-layer="246" from-port="2" to-layer="252" to-port="1" />
		<edge from-layer="247" from-port="0" to-layer="248" to-port="0" />
		<edge from-layer="248" from-port="1" to-layer="251" to-port="0" />
		<edge from-layer="249" from-port="0" to-layer="250" to-port="1" />
		<edge from-layer="250" from-port="2" to-layer="251" to-port="1" />
		<edge from-layer="251" from-port="2" to-layer="252" to-port="2" />
		<edge from-layer="252" from-port="3" to-layer="257" to-port="0" />
		<edge from-layer="253" from-port="0" to-layer="257" to-port="1" />
		<edge from-layer="254" from-port="0" to-layer="257" to-port="2" />
		<edge from-layer="255" from-port="0" to-layer="257" to-port="3" />
		<edge from-layer="256" from-port="0" to-layer="257" to-port="4" />
		<edge from-layer="257" from-port="5" to-layer="258" to-port="1" />
		<edge from-layer="258" from-port="2" to-layer="260" to-port="0" />
		<edge from-layer="259" from-port="0" to-layer="260" to-port="1" />
		<edge from-layer="260" from-port="2" to-layer="267" to-port="0" />
		<edge from-layer="261" from-port="0" to-layer="262" to-port="0" />
		<edge from-layer="262" from-port="1" to-layer="264" to-port="0" />
		<edge from-layer="263" from-port="0" to-layer="264" to-port="1" />
		<edge from-layer="264" from-port="2" to-layer="266" to-port="0" />
		<edge from-layer="265" from-port="0" to-layer="266" to-port="1" />
		<edge from-layer="266" from-port="2" to-layer="267" to-port="1" />
		<edge from-layer="267" from-port="2" to-layer="270" to-port="0" />
		<edge from-layer="268" from-port="0" to-layer="269" to-port="0" />
		<edge from-layer="269" from-port="1" to-layer="270" to-port="1" />
		<edge from-layer="270" from-port="2" to-layer="272" to-port="0" />
		<edge from-layer="271" from-port="0" to-layer="272" to-port="1" />
		<edge from-layer="272" from-port="2" to-layer="273" to-port="0" />
	</edges>
</net>
